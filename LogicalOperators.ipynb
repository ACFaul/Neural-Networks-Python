{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can neural networks learn logical operators?\n",
    "\n",
    "The simple answer is, yes, they can. Since all computing comes down to a combination of locigal operators, neural networks can emulate all computing. The efficiency is about the same as of those monkeys writing Shakespeare. The design of deep learning guides the process.\n",
    "\n",
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define logical operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(v1,v2):\n",
    "    if not (v1.all() in (0,1) and v2.all() in (0,1)):\n",
    "        raise ValueError(\"Input 0 or 1\")\n",
    "    return v1 | v2\n",
    "def AND(v1,v2):\n",
    "    if not (v1.all() in (0,1) and v2.all() in (0,1)):\n",
    "        raise ValueError(\"Input 0 or 1\")\n",
    "    return v1 & v2\n",
    "def NAND(v1,v2):\n",
    "    if not (v1.all() in (0,1) and v2.all() in (0,1)):\n",
    "        raise ValueError(\"Input 0 or 1\")\n",
    "    return ~(v1 & v2)+2\n",
    "def XOR(v1,v2):\n",
    "    if not (v1.all() in (0,1) and v2.all() in (0,1)):\n",
    "        raise ValueError(\"Input 0 or 1\")\n",
    "    return AND(OR(v1,v2),NAND(v1,v2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display as dataframe\n",
    "Each row is a sample, each column a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  v1  v2  OR  AND  NAND  XOR\n",
      "   0   0   0    0     1    0\n",
      "   0   1   1    0     1    1\n",
      "   1   0   1    0     1    1\n",
      "   1   1   1    1     0    0\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([0,0,1,1]).astype(np.uint8).reshape(-1,1)\n",
    "v2 = np.array([0,1,0,1]).astype(np.uint8).reshape(-1,1)\n",
    "data = np.hstack((v1, v2, OR(v1,v2), AND(v1,v2), NAND(v1,v2), XOR(v1,v2)))\n",
    "df = pandas.DataFrame(data)\n",
    "df.columns = ['v1', 'v2', 'OR', 'AND', 'NAND', 'XOR']\n",
    "df.index = ['' ,'', '', '']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "### Logistic sigmoid\n",
    "The logistic sigmoid function is defined as\n",
    "\\begin{equation}\n",
    "y = h(x) = \\frac{1}{1+\\exp(-x)} = \\frac{\\exp(x)}{\\exp(x) +1}\n",
    "\\end{equation}\n",
    "The derivative is\n",
    "\\begin{equation}\n",
    "h'(x) = \\frac{\\exp(x)}{(\\exp(x) +1)^2} = h(x) (1 - h(x)) = y(1-y)\n",
    "\\end{equation}\n",
    "This will be used as activation function. The interpretation is that the neuron fires for positive values, but does not react for negative values. Around zero is a transition zone between the two states. The size of the transition zone can be changed by multiplying $x$ by a positive number. The larger this number the faster the transition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-4,4,100)\n",
    "y = sigmoid(x)\n",
    "fig = plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network class\n",
    "We illustrate this with a simple two layer neural network with two input neurons and one output neuron and two neurons in the set of hidden neurons. The bias is incorporated in dummy neurons which always have the value 1.\n",
    "\n",
    "### Forward propagation\n",
    "![alt text](Forward.png \"Forward propagation\")\n",
    "\n",
    "The activations of the hidden neurons are calculated as\n",
    "\\begin{equation}\n",
    "\\left( \\begin{array}{c} a_1 \\\\ a_2 \\end{array} \\right) = \\left( \\begin{array}{ccc} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{array} \\right) \\left( \\begin{array}{c} x_1 \\\\ x_2 \\\\ 1 \\end{array} \\right).\n",
    "\\end{equation}\n",
    "The latent variables in the hidden neurons are then\n",
    "\\begin{equation}\n",
    "\\left( \\begin{array}{c} z_1 \\\\ z_2 \\end{array} \\right) = \\left( \\begin{array}{c} h(a_1) \\\\ h(a_2) \\end{array} \\right)\n",
    "\\end{equation}\n",
    "The activation of the output neuron is\n",
    "\\begin{equation}\n",
    "a = \\left( \\begin{array}{ccc} w_1 & w_2 & w_3 \\end{array} \\right) \\left( \\begin{array}{c} z_1 \\\\ z_2 \\\\ 1 \\end{array} \\right)\n",
    "\\end{equation}\n",
    "and the output is calculated as $y = h(a)$.\n",
    "\n",
    "### Training\n",
    "The prediction error is $\\delta = y-t$, where $t$ is the target. Training consists of minimizing a cost function $E$ which is calculated across all training data. Algorithmically, this minimization becomes the same for standard tasks (classification and regression) when the standard cost functions are used. (Note: care has to be taken when cost functions are modified). The negative gradient (derivative with respect to all weights) at a point on the cost function points in the direction of steepest descent. The minimization is performed by taking a step in this direction and adjusting the weights by this step. The step size is given by the learning rate $\\eta$, too small and the convergence is slow, too large and we might overshoot the minimum.\n",
    "\n",
    "### Backward error propagation\n",
    "![alt text](Backward.png \"Backward error propagation\")\n",
    "\n",
    "The error is $\\delta$ gets propagated back to the hidden neurons by\n",
    "\\begin{equation}\n",
    "\\left( \\begin{array}{c} \\delta_1 \\\\ \\delta_2 \\end{array} \\right) = \\delta \\left( \\begin{array}{c} h'(a_1) w_1 \\\\ h'(a_2) w_2 \\end{array} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "The derivatives of $E$ with respect to the weights are\n",
    "\\begin{equation}\n",
    "\\left( \\begin{array}{ccc} \\frac{\\partial}{\\partial w_1} & \\frac{\\partial}{\\partial w_2} & \\frac{\\partial}{\\partial w_3} \\end{array} \\right) E = \\delta \\left( \\begin{array}{ccc} z_1 & z_2 & 1 \\end{array} \\right)\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    "\\left(\\begin{array}{ccc} \\frac{\\partial}{\\partial w_{11}} & \\frac{\\partial}{\\partial w_{12}} & \\frac{\\partial}{\\partial w_{13}} \\\\ \\frac{\\partial}{\\partial w_{21}} & \\frac{\\partial}{\\partial w_{22}} & \\frac{\\partial}{\\partial w_{23}} \\end{array} \\right) E = \\left( \\begin{array}{c} \\delta_1 \\\\ \\delta_2 \\end{array} \\right) \\left( \\begin{array}{ccc} x_1 & x_2 & 1 \\end{array} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "After seeing one training sample, the weights are updated by\n",
    "\\begin{equation}\n",
    "\\left( \\begin{array}{ccc} w_1 & w_2 & w_3 \\end{array} \\right) = \\left( \\begin{array}{ccc} w_1 & w_2 & w_3 \\end{array} \\right) -  \\eta \\delta \\left( \\begin{array}{ccc} z_1 & z_2 & 1 \\end{array} \\right)\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\left( \\begin{array}{ccc} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{array} \\right) = \\left( \\begin{array}{ccc} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{array} \\right) -  \\eta \\left( \\begin{array}{c} \\delta_1 \\\\ \\delta_2 \\end{array} \\right) \\left( \\begin{array}{ccc} x_1 & x_2 & 1 \\end{array} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Using one training sample at a time, is known as *online* or *sequential* learning. Updating in one step using several training samples is known as *batch* or *mini-batch* methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nin,     # number of input neurons\n",
    "                 nout,    # number of output neurons\n",
    "                 nhidden, # number of hidden neurons\n",
    "                 tol,     #tolerance\n",
    "                 nepoch,  # number of epochs\n",
    "                 rate):   # learning rate\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.nhidden = nhidden\n",
    "        self.tol = tol\n",
    "        self.nepoch = nepoch\n",
    "        self.rate = rate \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):  # He et al.(2015) initialization\n",
    "        # + 1 is the bias neuron\n",
    "        self.weights_in = np.random.randn(self.nhidden, self.nin+1) * np.sqrt(2/self.nin)\n",
    "        self.weights_out = np.random.randn(self.nout, self.nhidden+1) * np.sqrt(2/self.nhidden)\n",
    "        \n",
    "    def predict(self, X): # X row vectors\n",
    "        # Append bias and multiply with weights.\n",
    "        Y = self.weights_in @ np.append(X.T,np.ones((1,X.shape[0])), axis = 0)\n",
    "        # Apply activation function.\n",
    "        Y = sigmoid(Y)\n",
    "        # Append bias and multiply with weights.\n",
    "        Y = self.weights_out @ np.append(Y,np.ones((1,Y.shape[1])), axis =0)\n",
    "        # Apply activation function.\n",
    "        Y = sigmoid(Y)\n",
    "        return Y.T\n",
    "    \n",
    "    def train(self, X, target):\n",
    "        # Permute the training data.\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        # Append bias, multiply with weights and apply activation function.\n",
    "        Z = sigmoid(self.weights_in @ np.append(X[perm,:].T,np.ones((1,X.shape[0])), axis = 0))\n",
    "        Y = sigmoid(self.weights_out @ np.append(Z,np.ones((1,Z.shape[1])), axis =0))\n",
    "        # Errors for all training samples.\n",
    "        Y_errors = Y - target[perm].T\n",
    "        cost = 0.5 * np.sum(Y_errors**2)\n",
    "        epoch = 0\n",
    "        while cost > self.tol and epoch < self.nepoch:\n",
    "            epoch = epoch+1\n",
    "            # Propagate errors. \n",
    "            # Note first multiplication componentwise, second matrix multiplication.\n",
    "            # Using special property of sigmoid derivative.\n",
    "            Z_errors = sigmoid_derivative(Z) * (self.weights_out[:,:-1].T @ Y_errors)\n",
    "            # Update weights in second layer.\n",
    "            self.weights_out = self.weights_out - self.rate * Y_errors @ np.append(Z,np.ones((1,Z.shape[1])), axis =0).T\n",
    "            # Update weights in first layer.\n",
    "            self.weights_in = self.weights_in - self.rate * Z_errors @ np.append(X[perm,:],np.ones((X.shape[0],1)), axis = 1)\n",
    "            # Permute the training data.\n",
    "            perm = np.random.permutation(X.shape[0])\n",
    "            # Append bias, multiply with weights and apply activation function.\n",
    "            Z = sigmoid(self.weights_in @ np.append(X[perm,:].T,np.ones((1,X.shape[0])), axis = 0))\n",
    "            Y = sigmoid(self.weights_out @ np.append(Z,np.ones((1,Z.shape[1])), axis =0))\n",
    "            # Errors for all training samples.\n",
    "            Y_errors = Y - target[perm].T\n",
    "            cost = 0.5 * np.sum(Y_errors**2)\n",
    "        return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning logical operators\n",
    "Choose one of the last four columns as target. A lot more epochs are necessary, to learn XOR. This is because, it is a combination of other logical operators. The other logical operators can actually be learned by an even simpler neural network consisting of only one layer, two input and one output neurons and **no** hidden neurons (left as exercise), because they only need to separate one region from another. XOR needs two layers, because it needs to separate three regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output before training:\n",
      "[[0.55974865]\n",
      " [0.54119159]\n",
      " [0.52095599]\n",
      " [0.51093086]]\n",
      "Epochs until convergence: 98\n",
      "Output after training rounded to the nearest integer:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "X = data[:,0:2]\n",
    "target = data[:,-1]\n",
    "NN = NeuralNetwork(nin=2, nout=1, nhidden=2, tol = 0.1, nepoch=100, rate=1)\n",
    "print('Output before training:')\n",
    "print(NN.predict(X))\n",
    "print('Epochs until convergence:', NN.train(X,target))\n",
    "print('Output after training rounded to the nearest integer:')\n",
    "print(np.round(NN.predict(X)).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
